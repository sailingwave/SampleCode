#!/bin/bash
#PBS -l walltime=1:00:00
#PBS -l nodes=1:ppn=8
#PBS -l mem=8gb
#PBS -M sailingwave@gmail.com
#PBS -N R_le_chr22_4
#PBS -j oe
#noPBS -q conti
#noPBS -A lc_dvc

#=========================#
# v1.02
# updated: 12/06/2014
# include sample size info in the result file. remove columns with names "." (in Carl's files, there are markers with names "."). Therefore, in the final result file, there might be fewer SNPs than are from the marker files.
#
# v1.01
# updated: 03/07/2014
# add a function: if the marker files are zipped, unzip it first and then run.
#
# updated: 08/02/2013
# changed how monomorphic is detected for lm/glm models.
#
# updated: 07/31/2013
# Modified the way for deciding monomorphic SNPs, now also working for LOF model, where the homozygote of major allele is coded as -1 instead of 0.
#
# updated: 07/26/2013
# Added test type: lm, logistic and gee, in addition to glmm. lm and logistic are for independent subjects, while gee and glmm are for correlated subjects, e.g. in families.
# Changed the name of the output to be R_assoc_chrxx, as well as the name of this script.
#
# created: 06/10/2013
# Use GWAF to analyze dichotomous trait. Apply logistic regression using GLMM.
# Modified from gwas_assoc_v2.02.pbs. Currently dropped test type;
#
#=========================#
# v2.02  updated:02/17/2013
# change the way this script sort files (from general-numeric to alphabetical).
#
# v2.01
# Used separate folders for storing the pedigree file and the phenotype file. Added an function to check the file transfering temp folder for existence. Changed the default pedigree file name. Renamed this script file from assoc_hpcc_v2.pbs to gwas_assoc_v2.01.pbs.
#
# Version 2.0
# -----------
# Dropped unused functions,optimize codes,added a function to sort files by id before joining them to form the phenotype file,added a mailing option,added a debug option,added a file storage server detection function, added function of wald's test in addition to LRT.
# updated: 01/18/2013
#
#=========================#
# Run in temp dir to drop disk I/O burden.
# updated: 05/10/2012
# Fix (cancel) the correction for ascertainment.Drop one asian from analysis.
# updated: 05/09/2012
# Run solar regression jobs in parallel.
# updated: 03/14/2012
# make dir on the destiny server(biolinux2),each trait has a separate folder. Output file contains the jobid of hpcc.
# created: 03/08/2012
# Now it's important to change the PBS options,since some of the vars in the pbs file come from these options.e.g. job name,ppn.
#=========================#

#=========================#
# How to use this script:
# -----------------------
# 1. According to your need,modify the PBS arguments,as the starting several lines of this script file, note:
#  1.1 usually, do not change the name ("#PBS -N" option);if modified,please keep the "chrxx" part.
#  1.2 if using our node, specify "#PBS -q watt",and set ppn=24.
#  1.3 if using public node, delete "#PBS -q watt" or change it to  "#noPBS -q watt", and set ppn=8 (I suppose majority of hpcc nodes are of this type).
# 2. If necessary, modify the arguments listed in the following 'argument setting' section, especially the 'more frequently changed arguments' part.
# 3. Qsub & good luck!
# ps: usually this script is called using another bash script "multi_job.sh", achieving multiple submissions.
#=========================#

#=========================#
# Preparations before using this script:
# 1. You need to have a phenotype file,with the 1st column being the 'id',the following columns are for each trait residuals. Make sure you already adjust for age,sex,(bmi or pfat),and populatioin structure fractions.
# 2. You need to have a pedigree file, several marker files organized by each chromosome (typically already provided).
# 3. You need to set up path on your file storing server (i.e. biolinux2) for storing the result files and output files. Also change the corresponding arguments accordingly. Seperate folders for each model,adjustment and trait will be automatically generated by this script.
# 4. You need to set up direct ssh w/o password to your file storing server (i.e. biolinux2) on hpc-login1,using the ssh-keygen command on hpc-login1. Google it or ask Nan for details.
#=========================#



#====== Script Start ======#

source /etc/profile


#====== Argument setting ======#
#---- by default ----#

chr=`echo $PBS_JOBNAME|sed 's/.*\(chr[^_]*_[0-9]*\).*/\1/'`
jobid=`echo $PBS_JOBID|sed 's/\([0-9]*\).*/\1/'`    #the id number of jobid
parll_lim=$(($PBS_NUM_PPN*$PBS_NUM_NODES))  #the number of parallelization,which is equal to the number of cores

pbsdir="$PBS_O_WORKDIR"    #where the pbs & sh script files are
wkdir="$TMPDIR/association"    #where all the work are processed,temp folder by each node
server="biolinux2"    #name of the server where to put all the result & output files

starttime=`date`

#---- by user ----#
#-more frequently changed arguments
model="add"    #the model,additive as "add", dominant as "dom", recessive as "rec"
adj='asb'    #adjustment,age-sex as "as",age-sex-bmi as "asb",age-sex-pfat as "asp"
trait='sqrtdi_r'    #traits being analyzed
covar=''    #list of covariates separated by comma
#[ "$adj" == "asb" ] && covar="$covar,bmi"
test="lm"    #can be selected from 'lm','logistic','gee' and 'glmm'

#-typically set for once

phendir="/home/rcf-40/wangn/second_GWAS/tripod"    #where your phenotype file is (with residuals)

phenfile="pheno_td_${adj}_150127.csv"    #the phenotype file that need to be combined with marker files, to generate the final 'phenotype' file for solar

outdir="/home/rcf-proj/rmw1/wangn/second_gwas/imputed_from_carl/result/tripod"

user='wangn'    #user name for $server,hint: usc user name
mailadd='sailingwave@gmail.com'    #email address that the a notification is sent to after finishing the job

#-default arguments that almost never need to be changed
peddir="/home/rcf-proj/rmw/BetaGene_Tripod_GWAS/pedigree"    #where the pedigree file is
mkrdir="/home/rcf-proj/rmw1/wangn/second_gwas/imputed_from_carl/tripod/${model}/done"    #where the marker files are

pedfile='betagene_tripod_130131.ped'    #pedigree file for solar
pref="tripod_${model}_imp_trp_141124_"    #the prefix of marker file: from beginning to where the "chrxx" starts;don't forget the suffix
suff=".csv.gz"    #the suffix of marker file
mkrfile="$pref$chr$suff"    #the name of the marker file

#---- job control ----#
mail=0    #whether send email notification after finishing the job, 1 or 0
debug=0    #debug mode: do not delete the intermediate files, 1 or 0
[ "$debug" == 1 ] && wkdir="/home/rcf-proj/rmw/wangn/temp"

#====== check  ======#
#---- necessary files ----#

if [ ! -e $peddir/$pedfile ] || [ ! -e $phendir/$phenfile ] || [ ! -e $mkrdir/$mkrfile ]; then
    echo "Make sure your input files (pedigree,phenotype,marker) exist!"
    exit
fi

if [ "$test" != "lm" -a "$test" != "logistic" -a "$test" != "gee" -a "$test" != "glmm" ];then
    echo "Make sure your test type is correct!"
    exit
fi

#====== PBS job decription ======#

echo ""
echo "Jobid: $PBS_JOBID"
echo "Model: $model"
echo "Adjust: $covar"
echo "Chr: $chr"
echo "Trait: $trait"
echo "Pedigree: $pedfile"
echo "Pheno: $phenfile"
echo "Marker: $mkrfile"
echo "Job starts"
echo "------------------------"


#====== File preparation ======#

#---- pbs parallel files ----#

chrdir="${chr}_$jobid";    #dir name to differentiate different traits for each chr

cd $pbsdir

[ -e "$chrdir" ] || mkdir $chrdir

cd $chrdir


#one file needed for parallelization
cat > $pbsdir/$chrdir/assoc_pbsdsh.sh <<EOF
#!/bin/bash
cd $pbsdir/$chrdir
sh a_\$PBS_VNODENUM.sh
EOF

#---- ped&pheno file ----#

[ ! -e $wkdir ] && mkdir $wkdir
cd $wkdir

if [ ! -e $chrdir ]; then
    mkdir $chrdir

    cp $peddir/$pedfile $phendir/$phenfile $mkrdir/$mkrfile $chrdir/
fi


cd $wkdir/$chrdir

if [[ "$suff" =~ ".gz" ]];then
    gzip -d $mkrfile
    mkrfile=${mkrfile%.gz}
fi

[ -e $mkrfile ] && count=$((`cat $mkrfile | grep rs | tr -s , "\n" | wc -l`-1)) || exit   #take out the first line & calculate;'-1' because of the 'id' column

limit=$(($count/$parll_lim))+1    #the number of SNPs for each core

#---- prepare subset pheno files & pbsdsh files ----#
sed -i 's///g' $phenfile    #delete the tailing newline sign by windows
m=${model:0:1}    #the first character of $model, used in GWAF

covarP=`echo $covar|sed 's/,/+/g'`    #to put into R's lm regression formula,with plus signs
covar="\"`echo $covar | sed 's/,/","/g'`\""    #format used in GWAF


i=1    #counter for snps
j=1    #counter for subset marker files & subset pheno files
while [ "$i" -le "$count" ];do
    #split marker file
    [ "$(($i+$limit-1))" -gt "$count" ] && uplimit=$count || uplimit=$(($i+$limit-1))
    chr_sub="${chr}_$j"
    cut -d , -f 1,$(($i+1))-$(($uplimit+1)) $mkrfile > $chr_sub.mkr    #+1,because the 1st col is id

    #-- write paralellization files --#
    cat > $pbsdir/$chrdir/a_$(($j-1)).sh<<EOF
#!/bin/bash
PATH=$PATH:/home/rcf-proj3/rmw/bin
export PATH
echo "Processing $chr_sub ..."
cd $wkdir/$chrdir

/home/rcf-proj/rmw/bin/R --slave <<EOF2

setwd("$wkdir/$chrdir")
library('data.table')

if("$test"=="lm"|"$test"=="logistic"){
  mkr<-fread("$chr_sub.mkr",sep=",",header=T)
  mkr[,grep('\\\\\\\\.',colnames(mkr)):=NULL]    #SNPs with a name of '.'
  mkr[,grep('-9',colnames(mkr)):=NULL]    #SNPs with a name of -9
  mkr[,grep('rs71904485',names(mkr)):=NULL]    #duplicated SNP
  pheno<-fread("$phenfile",sep=",",header=T)  #prevent the name to be corrected by R
  setkey(mkr,id)
  setkey(pheno,id)
  all<-merge(pheno,mkr)

  snp<-names(mkr)[-1]    #drop the 'id' column

  sink(file="$chr_sub.csv",type="output")    #output to file
  cat("phen,snp\n")    #consistent header format

  for (i in 1:length(snp)){
    if ("$covarP"==""){
      form<-as.formula(paste("\\\`$trait\\\`~",snp[i],sep=""))
    }else{
      form<-as.formula(paste("\\\`$trait\\\`~",snp[i],"+$covarP",sep=""))
    }

    if("$test"=="lm"){
      snplm<-lm(form,data=all)
    }else{
      snplm<-glm(form,data=all,family="binomial")
    }
    
    n = sum(summary(snplm)\\\$df[1:2])  #sample size

    if (is.na(snplm\\\$coef[2])){    #monomorphic
      cat(paste(snp[i]," $trait bsnp = 0 se = 0 p = 1 n = ",n,"\n",sep=""))
    } else {
      cat(paste(snp[i]," $trait bsnp = ",summary(snplm)\\\$coef[2,1]," se = ",summary(snplm)\\\$coef[2,2]," p = ",summary(snplm)\\\$coef[2,4]," n = ",n,"\n",sep=""))
    }
  }

  sink()
}else if ("$test"=="gee"){
  library('GWAF')
  gee.lgst.batch(phenfile="$phenfile",genfile="$chr_sub.mkr",pedfile="$pedfile",phen="$trait",model="$m",outfile="$chr_sub.csv",covars=c($covar),sep.ped=",",sep.phe=",",sep.gen=",")
}else if ("$test"=="glmm"){
  library('GWAF')
  glmm.lgst.batch(phenfile="$phenfile",genfile="$chr_sub.mkr",pedfile="$pedfile",phen="$trait",model="$m",outfile="$chr_sub.csv",covars=c($covar),sep.ped=",",sep.phe=",",sep.gen=",")
}else{
  print("wrong test type!")
}    
EOF2

EOF
  
	i=$(($i+$limit))
	j=$(($j+1))
done

#--------------------------------------------------------#



#====== parallelization ======#

cd $pbsdir

pbsdsh -v sh $pbsdir/$chrdir/assoc_pbsdsh.sh

#=============================#



#====== Job done ======#

#---- job summarization & transfer files ----#


finishtime=`date`

cd $wkdir/$chrdir

head -n 1 ${chr}_1.csv > result_${chr}_$jobid.csv
cat $chr*csv | grep -v "^phen,snp" >> result_${chr}_$jobid.csv
gzip -9 result_${chr}_$jobid.csv

[ ! -e $outdir/$model/$adj/$trait ] && mkdir -p $outdir/$model/$adj/$trait 
mv result_${chr}_$jobid.csv.gz $outdir/$model/$adj/$trait/ 


if [ "$debug" == "0" ];then
    rm -rf $pbsdir/$chrdir
    rm -rf $wkdir/$chrdir
fi

#---- finishing ----#

if [ "$mail" == "1" ];then
    ssh hpc-login1 "mail -s 'PBS association analysis for $chr done!' $mailadd<<EOF
jobid: $PBS_JOBID
trait: $trait
start: $starttime
finish: $finishtime
EOF"
fi

